{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# import pytorch_DIW_scratch\n",
    "import torch.nn as nn\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "# from blurgenerator import motion_blur, lens_blur, gaussian_blur\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import pandas as pd\n",
    "import PIL.Image as pil\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "top = T.ToPILImage()\n",
    "tot = T.ToTensor()\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# sys.path.insert(1,'PerceptualSimilarity')\n",
    "# import models\n",
    "import unet_parts as util\n",
    "import lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "from unet_parts import *\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels_org, n_channels_depth, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels_org = n_channels_org\n",
    "        self.n_channels_depth = n_channels_depth\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc_org = (DoubleConv(n_channels_org, 32))\n",
    "        self.inc_depth = (DoubleConv(n_channels_depth, 32))\n",
    "\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "\n",
    "        self.outc_0 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False), # 3, 32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False), # 3, 32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, depth, org):\n",
    "        d1 = self.inc_depth(depth)\n",
    "        org1 = self.inc_org(org)\n",
    "\n",
    "        x1 = torch.cat([d1, org1], axis=1) # (b, 64, w, h)\n",
    "\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc_0(x)\n",
    "        x = self.outc(x)\n",
    "\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = UNet(3, 1, 4).cuda()\n",
    "\n",
    "\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "# sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "# 4125956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Soester10/Bokeh-Rendering-with-Vision-Transformers/blob/main/train.py\n",
    "\n",
    "import PIL.Image as pil\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# feed_width = 768\n",
    "# feed_height =  512\n",
    "# feed_height = 384\n",
    "# feed_width = 512\n",
    "# feed_height = 768\n",
    "# feed_width = 1024\n",
    "\n",
    "feed_height = 1024\n",
    "feed_width = 1536\n",
    "\n",
    "batch_size = 1#0\n",
    "\n",
    "\n",
    "class bokehDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file,root_dir, transform=None):\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        bok = pil.open(self.root_dir + self.data.iloc[idx, 0][1:]).convert('RGB')\n",
    "        org = pil.open(self.root_dir + self.data.iloc[idx, 1][1:]).convert('RGB')\n",
    "        original_width, original_height = org.size\n",
    "\n",
    "        depth_path = str(self.root_dir + self.data.iloc[idx, 1][1:]\n",
    "                         ).replace('original', 'depth').replace('jpg', 'png')\n",
    "        depth = pil.open(depth_path)\n",
    "\n",
    "        blur_6_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_6_3_2')\n",
    "        blur_17_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_17_3_2')\n",
    "        blur_65_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_65_3_3')\n",
    "        blur_6_path = pil.open(blur_6_path).convert('RGB')\n",
    "        blur_17_path = pil.open(blur_17_path).convert('RGB')\n",
    "        blur_65_path = pil.open(blur_65_path).convert('RGB')\n",
    "\n",
    "        bok = bok.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "        org = org.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "        depth = depth.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "\n",
    "        blur_6_path = blur_6_path.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "        blur_17_path = blur_17_path.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "        blur_65_path = blur_65_path.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "\n",
    "\n",
    "        if self.transform : \n",
    "            bok_dep = self.transform(bok)\n",
    "            org_dep = self.transform(org)\n",
    "            depth_dep = self.transform(depth)\n",
    "\n",
    "            blur_6_path = self.transform(blur_6_path)\n",
    "\n",
    "            blur_17_path = self.transform(blur_17_path)\n",
    "\n",
    "            blur_65_path = self.transform(blur_65_path)\n",
    "        \n",
    "        stacked_10 = torch.stack([org_dep, blur_6_path, blur_17_path, blur_65_path], dim=0)\n",
    "        return (bok_dep, org_dep, depth_dep, stacked_10, self.root_dir + self.data.iloc[idx, 1][1:], \n",
    "                original_width, original_height) # here\n",
    "\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "testset = bokehDataset(csv_file = '../MegaDepth/Bokeh_Data/test.csv',  root_dir = '.', transform = transform1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/ubuntu/max/image/JPEG/C++_pipeline/zip/depth_estimation/DPT/PerceptualSimilarity/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lpips\n",
    "\n",
    "device = 'cuda'\n",
    "                                                  \n",
    "MSE_LossFn = nn.MSELoss()\n",
    "L1_LossFn = nn.L1Loss()\n",
    "lpips_fn = lpips.LPIPS(net='alex')#.cuda() # models.PerceptualLoss(model='net-lin',net='alex',use_gpu=opt.use_gpu)\n",
    "\n",
    "parameters_path = '/home/ubuntu/max/image/JPEG/C++_pipeline/zip/depth_estimation/DPT/weights/top_4_blurs_finetune/55.pt'\n",
    "parameters = torch.load(parameters_path, map_location=torch.device(\"cpu\"))\n",
    "model.load_state_dict(parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 294/294 [01:15<00:00,  3.91it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    lpips_fn = lpips_fn.cuda()\n",
    "    running_l1_loss = 0\n",
    "    running_ms_loss = 0\n",
    "    running_lpips_loss = 0\n",
    "\n",
    "    for i,data in enumerate(tqdm(testloader), 0):\n",
    "        device = 'cuda'\n",
    "        bok, org, depth, stacked_10, path, original_width, original_height = data\n",
    "        bok, org, depth = bok.to(device) , org.to(device), depth.to(device, dtype=torch.float)\n",
    "        stacked_10 = stacked_10.to(device)\n",
    "\n",
    "        # bok_mask = model.forward(depth)\n",
    "        bok_mask = model.forward(depth, org)\n",
    "\n",
    "        # stacked_images = torch.stack([org, blur_25, blur_45, blur_75], dim=1).cuda()  # [batch_n, 4, 3, h, w]\n",
    "        # stacked_images = torch.stack([org, blur_45], dim=1).cuda()  # [batch_n, 4, 3, h, w]\n",
    "\n",
    "\n",
    "        # Multiply the maps and images using broadcasting\n",
    "        bok_mask = bok_mask.unsqueeze(2)\n",
    "        output_images = torch.mul(bok_mask, stacked_10) # [batch_n, 4, 3, h, w]\n",
    "        bok_pred = torch.sum(output_images, dim=1) # [batch_n, 3, h, w]\n",
    "\n",
    "\n",
    "        \n",
    "        l1_loss = L1_LossFn(bok_pred, bok)\n",
    "        ms_loss = 1-ssim(bok_pred, bok)\n",
    "        lpips_loss = lpips_fn.forward(bok_pred, bok)\n",
    "\n",
    "        running_l1_loss += l1_loss.item()\n",
    "        running_ms_loss += ms_loss.item()\n",
    "        running_lpips_loss += lpips_loss.item()\n",
    "\n",
    "\n",
    "        # top(bok_pred[0]).save(path)\n",
    "        # print('before', bok_pred.shape)\n",
    "        bok_pred = F.interpolate(bok_pred,(original_height,original_width),mode = 'bilinear')\n",
    "\n",
    "        path = path[0].replace('original', 'val294_55')\n",
    "        # path_bok = path.replace('val294', 'bok294')\n",
    "\n",
    "        top(bok_pred[0]).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_l1_loss / 294, running_ms_loss, running_lpips_loss / 294\n",
    "# # (0.04888411183670467, 0.21529191732406616, 0.10059311754089229) 42\n",
    "# # (0.0483693949003913,  0.2130470871925354,   0.09418721285452243)   45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/opt/anaconda3/envs/max_2_0/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/max/opt/anaconda3/envs/max_2_0/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/max/opt/anaconda3/envs/max_2_0/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/294 [00:00<?, ?it/s]/var/folders/7b/dxvk6ctn7yzgkzt7ccsrrtgh0000gn/T/ipykernel_86629/3166847299.py:42: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  total_ssim += compare_ssim(I0,I1,multichannel=True, channel_axis=2)\n",
      "100%|██████████| 294/294 [06:22<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg LPIPS:  0.1880474775193297\n",
      "Avg PSNR:  24.019705778397935\n",
      "Avg SSIM:  0.8638111441949295\n",
      "Total files:  294\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'parameters_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7b/dxvk6ctn7yzgkzt7ccsrrtgh0000gn/T/ipykernel_86629/3166847299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Avg SSIM: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_ssim\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Total files: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Weights:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parameters_path' is not defined"
     ]
    }
   ],
   "source": [
    "dir0 = '/Users/max/Documents/diploma/server/variable_blur_radius/4_power'\n",
    "# dir0 = '/home/ubuntu/max/image/JPEG/C++_pipeline/zip/DMSHN/val294'\n",
    "dir1 = '/Users/max/Documents/diploma/server/val/val294_bok'\n",
    "# dir1 = '/home/ubuntu/max/image/JPEG/C++_pipeline/zip/DMSHN/bok294_dmshn'\n",
    "\n",
    "## Initializing the model\n",
    "import lpips\n",
    "model = lpips.LPIPS(net='alex') # models.PerceptualLoss(model='net-lin',net='alex',use_gpu=opt.use_gpu)\n",
    "model = model#.cuda()\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import unet_parts as util\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "\n",
    "# crawl directories\n",
    "files = os.listdir(dir0)\n",
    "\n",
    "total_dist = 0\n",
    "total_psnr = 0\n",
    "total_ssim = 0\n",
    "count =0\n",
    "\n",
    "for file in tqdm(files):\n",
    "\tfile1 = file[:4] + '.jpg'\n",
    "\tif(os.path.exists(os.path.join(dir1,file1))):\n",
    "\t\t# Load images\n",
    "\t\timg0 = util.im2tensor(util.load_image(os.path.join(dir0,file))) # RGB image from [-1,1]\n",
    "\t\timg1 = util.im2tensor(util.load_image(os.path.join(dir1,file1)))\n",
    "\t\t# img0 = img0.cuda()\n",
    "\t\t# img1 = img1.cuda()\n",
    "\n",
    "\t\t# Compute distance\n",
    "\t\tdist01 = model.forward(img0,img1)\n",
    "\t\ttotal_dist += dist01.item()\n",
    "\n",
    "\t\tI0 = cv2.imread(os.path.join(dir0,file))\n",
    "\t\tI1 = cv2.imread(os.path.join(dir1,file1))\n",
    "\n",
    "\t\ttotal_psnr += compare_psnr(I0,I1)\n",
    "\t\ttotal_ssim += compare_ssim(I0,I1,multichannel=True, channel_axis=2)\n",
    "\n",
    "\t\tcount +=1\n",
    "\n",
    "print ('Avg LPIPS: ', total_dist/len(files))\n",
    "print ('Avg PSNR: ', total_psnr/len(files))\n",
    "print ('Avg SSIM: ', total_ssim/len(files))\n",
    "print ('Total files: ', count)\n",
    "print (\"Weights:\", parameters_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15\n",
    "# Avg LPIPS:  0.25091465978192634\n",
    "# Avg PSNR:  23.932834724123268\n",
    "# Avg SSIM:  0.8544052056915824\n",
    "\n",
    "# 23\n",
    "# Avg LPIPS:  0.24972314648583632\n",
    "# Avg PSNR:  24.100258935649894\n",
    "# Avg SSIM:  0.8538297881025634\n",
    "\n",
    "# 40\n",
    "# Avg LPIPS:  0.2051788743357269\n",
    "# Avg PSNR:  23.915384389011518\n",
    "# Avg SSIM:  0.8607180569365682\n",
    "\n",
    "# 50\n",
    "# Avg LPIPS:  0.1780613246468865\n",
    "# Avg PSNR:  24.244362704694105\n",
    "# Avg SSIM:  0.8666627774577712\n",
    "\n",
    "# 55\n",
    "# Avg LPIPS:  0.17861873519663907\n",
    "# Avg PSNR:  24.26308359282466\n",
    "# Avg SSIM:  0.8672498382263079\n",
    "\n",
    "# 56\n",
    "# Avg LPIPS:  0.17850339405086577\n",
    "# Avg PSNR:  24.262182775017312\n",
    "# Avg SSIM:  0.8669904421732371\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_images_except(directory, keep_filenames):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".jpg\") and filename not in keep_filenames:\n",
    "                os.remove(os.path.join(root, filename))\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/Users/max/Documents/diploma/server/variable_blur_radius\"\n",
    "keep_filenames = [\"4507.jpg\", \"4682.jpg\"]\n",
    "\n",
    "delete_images_except(directory_path, keep_filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mine\n",
    "# Avg LPIPS:  0.17618930433261193\n",
    "# Avg PSNR:  24.200597489832337\n",
    "# Avg SSIM:  0.8668268319166299\n",
    "\n",
    "# 4 power\n",
    "\n",
    "# BGGAN\n",
    "# Avg LPIPS:  0.1473597307138297\n",
    "# Avg PSNR:  25.969951218691634\n",
    "# Avg SSIM:  0.8789945434643803\n",
    "\n",
    "\n",
    "\n",
    "# DMSHN\n",
    "# Avg LPIPS:  0.22179737745397757\n",
    "# Avg PSNR:  24.631763065227975\n",
    "# Avg SSIM:  0.8703852476185571\n",
    "\n",
    "# SDMSHN\n",
    "# Avg LPIPS:  0.21703358799168448\n",
    "# Avg PSNR:  24.700831853744432\n",
    "# Avg SSIM:  0.873192968526886\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 50 finetune lpips + ssim + L1 interpolate\n",
    "# Avg LPIPS:  0.1780613246468865\n",
    "# Avg PSNR:  24.244362704694105\n",
    "# Avg SSIM:  0.8666627774577712\n",
    "\n",
    "# Epoch 45 finetune lpips + ssim + L1 interpolate\n",
    "# Avg LPIPS:  0.18527495844581096\n",
    "# Avg PSNR:  24.15010095669144\n",
    "# Avg SSIM:  0.8644536434417619\n",
    "\n",
    "\n",
    "# Epoch 42 finetune lpips + ssim + L1 interpolate\n",
    "# Avg LPIPS:  0.20383347660860643\n",
    "# Avg PSNR:  23.9200654617216\n",
    "# Avg SSIM:  0.8610615226189505\n",
    "\n",
    "\n",
    "# Stacked DMSHN\n",
    "# Avg LPIPS:  0.2529120883067786\n",
    "# Avg PSNR:  28.237889625140056\n",
    "# Avg SSIM:  0.915850495268047\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Epoch 42 finetune lpips + ssim + L1\n",
    "# Avg LPIPS:  0.18566639696051473\n",
    "# Avg PSNR:  23.916317089155488\n",
    "# Avg SSIM:  0.8598279403285225\n",
    "\n",
    "\n",
    "# Epoch 30 finetune lpips + ssim + L1\n",
    "# Avg LPIPS:  0.19235397890514258\n",
    "# Avg PSNR:  23.758934071366454\n",
    "# Avg SSIM:  0.8574985115548894\n",
    "\n",
    "\n",
    "\n",
    "# Epoch 26 finetune lpips\n",
    "# Avg LPIPS:  0.20650175170732193\n",
    "# Avg PSNR:  23.685084036762046\n",
    "# Avg SSIM:  0.8530066391542362\n",
    "\n",
    "\n",
    "# Last epoch 22\n",
    "# Avg LPIPS:  0.23184171818247457\n",
    "# Avg PSNR:  23.995127629918684\n",
    "# Avg SSIM:  0.8532735757485003\n",
    "\n",
    "\n",
    "# Last epoch 23/24\n",
    "# Avg LPIPS:  0.22942917664744417\n",
    "# Avg PSNR:  24.092180066127206\n",
    "# Avg SSIM:  0.8524654714009784\n",
    "\n",
    "# Last epoch 27\n",
    "# Avg LPIPS:  0.2635695703175603\n",
    "# Avg PSNR:  24.05714959195201\n",
    "# Avg SSIM:  0.8466768354282206\n",
    "\n",
    "\n",
    "# Last epoch 29\n",
    "# Avg LPIPS:  0.27070816129851505\n",
    "# Avg PSNR:  24.08397440610926\n",
    "# Avg SSIM:  0.8443834677619755\n",
    "\n",
    "# Last epoch 30\n",
    "# Avg LPIPS:  0.26866581653352495\n",
    "# Avg PSNR:  24.107443187852514\n",
    "# Avg SSIM:  0.8456310957087362\n",
    "\n",
    "\n",
    "# Last epoch 32\n",
    "# Avg LPIPS:  0.28595100530759004\n",
    "# Avg PSNR:  24.050488337605206\n",
    "# Avg SSIM:  0.8419934323655651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/ubuntu/anaconda3/envs/max_2_0/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 14/294 [00:20<06:53,  1.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/max/image/JPEG/C++_pipeline/zip/depth_estimation/DPT/calculate_metrics.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/calculate_metrics.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/calculate_metrics.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor() \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/calculate_metrics.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(tqdm(executor\u001b[39m.\u001b[39;49mmap(process_file, files), total\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(files)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/calculate_metrics.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m dist, psnr, ssim, valid \u001b[39min\u001b[39;00m results:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/calculate_metrics.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     total_dist \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dist\n",
      "File \u001b[0;32m~/anaconda3/envs/max_2_0/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/max_2_0/lib/python3.8/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m fs\u001b[39m.\u001b[39;49mpop()\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m fs\u001b[39m.\u001b[39mpop()\u001b[39m.\u001b[39mresult(end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/anaconda3/envs/max_2_0/lib/python3.8/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 439\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/max_2_0/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "dir0 = 'Training/val294'\n",
    "# dir0 = '/home/ubuntu/max/image/JPEG/C++_pipeline/zip/DMSHN/val294'\n",
    "dir1 = 'Training/val294_bok'\n",
    "\n",
    "# Initialize the model\n",
    "model = lpips.LPIPS(net='alex')\n",
    "model = model#.cuda()\n",
    "\n",
    "# Crawl directories\n",
    "files = os.listdir(dir0)\n",
    "\n",
    "total_dist = 0\n",
    "total_psnr = 0\n",
    "total_ssim = 0\n",
    "count = 0\n",
    "\n",
    "\n",
    "def process_file(file):\n",
    "    file1 = file[:4] + '.jpg'\n",
    "    if os.path.exists(os.path.join(dir1, file1)):\n",
    "        # Load images\n",
    "        img0 = util.im2tensor(util.load_image(os.path.join(dir0, file)))  # RGB image from [-1, 1]\n",
    "        img1 = util.im2tensor(util.load_image(os.path.join(dir1, file1)))\n",
    "\n",
    "        # img0 = img0.cuda()\n",
    "        # img1 = img1.cuda()\n",
    "\n",
    "        # Compute distance\n",
    "        dist01 = model.forward(img0, img1)\n",
    "\n",
    "        I0 = cv2.imread(os.path.join(dir0, file))\n",
    "        I1 = cv2.imread(os.path.join(dir1, file1))\n",
    "\n",
    "        psnr = compare_psnr(I0, I1)\n",
    "        ssim = compare_ssim(I0, I1, multichannel=True, channel_axis=2)\n",
    "\n",
    "        return dist01.item(), psnr, ssim, 1\n",
    "    return 0, 0, 0, 0\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(process_file, files), total=len(files)))\n",
    "\n",
    "for dist, psnr, ssim, valid in results:\n",
    "    total_dist += dist\n",
    "    total_psnr += psnr\n",
    "    total_ssim += ssim\n",
    "    count += valid\n",
    "\n",
    "print('Avg LPIPS: ', total_dist / count)\n",
    "print('Avg PSNR: ', total_psnr / count)\n",
    "print('Avg SSIM: ', total_ssim / count)\n",
    "print('Total files: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg LPIPS:  0.24802155055257738\n",
    "# Avg PSNR:  24.237881301361597\n",
    "# Avg SSIM:  0.850600642181404\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "max_2_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
