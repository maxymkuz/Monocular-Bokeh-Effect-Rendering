{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/max_2_0/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "from dpt.models import DPTDepthModel\n",
    "from dpt.transforms import Resize, NormalizeImage, PrepareForNet\n",
    "import util.io\n",
    "from lens_blur import lens_blur\n",
    "from PIL import Image\n",
    "top = T.ToPILImage()\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from lens_blur import lens_blur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "# https://github.com/Soester10/Bokeh-Rendering-with-Vision-Transformers/blob/main/train.py\n",
    "\n",
    "import PIL.Image as pil\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "# from pytorch_msssim.pytorch_msssim import msssim\n",
    "# from pytorch_ssim.pytorch_ssim import ssim\n",
    "# import PerceptualSimilarity.lpips.lpips as lpips\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# feed_width = 768\n",
    "# feed_height =  512\n",
    "# feed_height = 384\n",
    "# feed_width = 512\n",
    "batch_size = 1#0\n",
    "\n",
    "\n",
    "class bokehDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file,root_dir, transform=None):\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        bok = pil.open(self.root_dir + self.data.iloc[idx, 0][1:]).convert('RGB')\n",
    "        org = pil.open(self.root_dir + self.data.iloc[idx, 1][1:]).convert('RGB')\n",
    "\n",
    "        depth_path = str(self.root_dir + self.data.iloc[idx, 1][1:]\n",
    "                         ).replace('original', 'depth').replace('jpg', 'png')\n",
    "        depth = pil.open(depth_path)\n",
    "\n",
    "        # bok = bok.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "        # org = org.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "        # depth = depth.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "\n",
    "        blur_6_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_6_3_2')\n",
    "        # blur_10_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_10_3_3')\n",
    "        blur_10_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_10_3_2')\n",
    "        blur_15_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_20_3_3')\n",
    "        # blur_15_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_15_3_1')\n",
    "        blur_30_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_30_3_5')\n",
    "        blur_65_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_50_2_4')\n",
    "        # blur_65_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_65_3_3')\n",
    "        blur_100_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_100_3_3')\n",
    "\n",
    "        blur_17_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_17_3_2')\n",
    "\n",
    "        # if not os.path.exists(blur_17_path):\n",
    "        #     blur = Image.fromarray(lens_blur(np.array(org), radius=17, components=5, exposure_gamma=2))\n",
    "        #     blur.save(blur_17_path)\n",
    "\n",
    "        # if not os.path.exists(blur_6_path):\n",
    "        #     blur = Image.fromarray(lens_blur(np.array(org), radius=6, components=5, exposure_gamma=2))\n",
    "        #     blur.save(blur_6_path)\n",
    "\n",
    "        # if not os.path.exists(blur_10_path):\n",
    "        #     blur = Image.fromarray(lens_blur(np.array(org), radius=10, components=5, exposure_gamma=3))\n",
    "        #     blur.save(blur_10_path)\n",
    "        \n",
    "        # if not os.path.exists(blur_15_path):\n",
    "        #     blur = Image.fromarray(lens_blur(np.array(org), radius=15, components=5, exposure_gamma=1))\n",
    "        #     blur.save(blur_15_path)\n",
    "\n",
    "        # if not os.path.exists(blur_30_path):\n",
    "        #     blur = Image.fromarray(lens_blur(np.array(org), radius=30, components=5, exposure_gamma=5))\n",
    "        #     blur.save(blur_30_path)\n",
    "\n",
    "        # if not os.path.exists(blur_65_path):\n",
    "        blur = Image.fromarray(lens_blur(np.array(org), radius=65, components=4, exposure_gamma=3))\n",
    "        blur.save(blur_65_path)\n",
    "\n",
    "        # if not os.path.exists(blur_100_path):\n",
    "        #     blur = Image.fromarray(lens_blur(np.array(org), radius=100, components=5, exposure_gamma=3))\n",
    "        #     blur.save(blur_100_path)\n",
    "\n",
    "        bok = self.transform(bok)\n",
    "\n",
    "        return (bok)\n",
    "\n",
    "        if self.transform : \n",
    "            bok_dep = self.transform(bok)\n",
    "            org_dep = self.transform(org)\n",
    "            depth_dep = self.transform(depth)\n",
    "\n",
    "            blur_25 = self.transform(blur_25)\n",
    "            blur_45 = self.transform(blur_45)\n",
    "            blur_75 = self.transform(blur_75)\n",
    "\n",
    "        depth_dep = depth_dep / depth_dep.max()\n",
    "\n",
    "        return (bok_dep, org_dep, depth_dep, blur_15, blur_25, blur_50)\n",
    "\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "transform3 = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(p=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "trainset1 = bokehDataset(csv_file = '../MegaDepth/Bokeh_Data/train.csv', root_dir = '.',transform = transform1)\n",
    "trainset2 = bokehDataset(csv_file = '../MegaDepth/Bokeh_Data/train.csv', root_dir = '.',transform = transform2)\n",
    "trainset3 = bokehDataset(csv_file = '../MegaDepth/Bokeh_Data/train.csv', root_dir = '.',transform = transform3)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([trainset1,trainset2,trainset3]), batch_size=batch_size,\n",
    "#                                           shuffle=True, num_workers=7)\n",
    "trainloader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([trainset1]), batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=6)\n",
    "\n",
    "testset = bokehDataset(csv_file = '../MegaDepth/Bokeh_Data/test.csv',  root_dir = '.', transform = transform1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=6)\n",
    "\n",
    "# blur_25_transform = T.GaussianBlur((25, 25), sigma=1)\n",
    "# blur_45_transform = T.GaussianBlur((45, 45), sigma=3)\n",
    "# blur_75_transform = T.GaussianBlur((75, 75), sigma=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 98/294 [01:47<03:12,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 294/294 [06:03<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, data in enumerate(tqdm(testloader), 0):\n",
    "    if i == 100:\n",
    "        print(i, '/', len(testloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- 3.3/it\n",
    "# 4- 1 per iter    1.07 50\n",
    "# 6- 1.05\n",
    "# 7- 1.12 per iter\n",
    "# 8- 1.42\n",
    "\n",
    "# 4 batch: 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DPTDepthModel(\n",
    "    path=None,# './weights/dpt_hybrid-midas-501f0c75.pt',\n",
    "    backbone=\"vitb_rn50_384\",\n",
    "    non_negative=True,\n",
    "    enable_attention_hooks=False,\n",
    ")\n",
    "\n",
    "parameters = torch.load('./weights/dpt_hybrid-midas-501f0c75.pt', map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# parameters.pop('scratch.output_conv.4.weight')\n",
    "# parameters.pop('scratch.output_conv.4.bias')\n",
    "\n",
    "\n",
    "model.load_state_dict(parameters)\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     model,\n",
    "#     nn.ReLU(True),\n",
    "#     nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n",
    "\n",
    "#     nn.ReLU(True),\n",
    "#     nn.Identity(),\n",
    "# )\n",
    "\n",
    "# model._modules['scratch']._modules['output_conv'] = nn.Sequential(\n",
    "#     model._modules['scratch']._modules['output_conv'],\n",
    "#     nn.ReLU(True),\n",
    "#     nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=0),\n",
    "\n",
    "#     nn.ReLU(True),\n",
    "#     nn.Identity(),\n",
    "# )\n",
    "\n",
    "# model\n",
    "# model = model.to(memory_format=torch.channels_last)\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "net_w = 768\n",
    "net_h = 576 # 384\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        Resize(\n",
    "            net_w,\n",
    "            net_h,\n",
    "            resize_target=None,\n",
    "            keep_aspect_ratio=True,\n",
    "            ensure_multiple_of=32,\n",
    "            resize_method=\"minimal\",\n",
    "            image_interpolation_method=cv2.INTER_CUBIC,\n",
    "        ),\n",
    "        normalization,\n",
    "        PrepareForNet(),\n",
    "    ]\n",
    ")\n",
    "model = torch.compile(model);\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/max/image/JPEG/C++_pipeline/zip/depth_estimation/DPT/generate_depth_lens_blur.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/generate_depth_lens_blur.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m img \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mread_image(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./Training/bokeh/\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/generate_depth_lens_blur.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m img_input \u001b[39m=\u001b[39m transform({\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m: img})[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/generate_depth_lens_blur.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m sample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(img_input)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/generate_depth_lens_blur.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m sample \u001b[39m=\u001b[39m sample\u001b[39m.\u001b[39mto(memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mchannels_last)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/generate_depth_lens_blur.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m sample \u001b[39m=\u001b[39m sample\u001b[39m.\u001b[39mcuda()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(4694):\n",
    "    with torch.no_grad():\n",
    "        if i % 200 == 1:\n",
    "            print(i)\n",
    "        filename = f\"{i}.jpg\"\n",
    "\n",
    "        img = util.io.read_image(f'./Training/bokeh/{filename}')\n",
    "\n",
    "        img_input = transform({\"image\": img})[\"image\"]\n",
    "\n",
    "\n",
    "        sample = torch.from_numpy(img_input).to(device).unsqueeze(0)\n",
    "        sample = sample.to(memory_format=torch.channels_last)\n",
    "\n",
    "        sample = sample.cuda()\n",
    "\n",
    "\n",
    "        prediction = model.forward(sample)\n",
    "\n",
    "        prediction = prediction / prediction.max()\n",
    "\n",
    "        prediction1 = (\n",
    "            torch.nn.functional.interpolate(\n",
    "                prediction.unsqueeze(1),\n",
    "                size=img.shape[:2],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            .squeeze()\n",
    "            .cpu().detach()   #!!!!!!!!!!!!!\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        # prediction.shape\n",
    "        util.io.write_depth(f\"./Training/depth/{i}\", prediction1, bits=2, absolute_depth=False)\n",
    "\n",
    "        torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "max_2_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
