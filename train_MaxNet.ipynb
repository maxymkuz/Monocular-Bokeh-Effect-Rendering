{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/max_2_0/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# import pytorch_DIW_scratch\n",
    "import torch.nn as nn\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "# from blurgenerator import motion_blur, lens_blur, gaussian_blur\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "top = T.ToPILImage()\n",
    "tot = T.ToTensor()\n",
    "\n",
    "from lens_blur import lens_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4199940"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "from unet_parts import *\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels_org, n_channels_depth, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels_org = n_channels_org\n",
    "        self.n_channels_depth = n_channels_depth\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc_org = (DoubleConv(n_channels_org, 32))\n",
    "        self.inc_depth = (DoubleConv(n_channels_depth, 32))\n",
    "\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "\n",
    "        self.outc_0 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False), # 3, 32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False), # 3, 32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, depth, org):\n",
    "        d1 = self.inc_depth(depth)\n",
    "        org1 = self.inc_org(org)\n",
    "\n",
    "        x1 = torch.cat([d1, org1], axis=1) # (b, 64, w, h)\n",
    "\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc_0(x)\n",
    "        x = self.outc(x)\n",
    "\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = UNet(3, 1, 4).cuda()\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "sum([np.prod(p.size()) for p in model_parameters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 384, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipt_depth = torch.rand([1, 1, 384, 512]).cuda()\n",
    "ipt_image = torch.rand([1, 3, 384, 512]).cuda()\n",
    "\n",
    "# # bokehNN.eval()\n",
    "res = model.forward(ipt_depth, ipt_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/Soester10/Bokeh-Rendering-with-Vision-Transformers/blob/main/train.py\n",
    "\n",
    "import PIL.Image as pil\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# from pytorch_msssim.pytorch_msssim import msssim\n",
    "# from pytorch_ssim.pytorch_ssim import ssim\n",
    "# import PerceptualSimilarity.lpips.lpips as lpips\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# feed_width = 768\n",
    "# feed_height =  512\n",
    "\n",
    "# feed_height = 384\n",
    "# feed_width = 512\n",
    "# feed_height = 768\n",
    "# feed_width = 1024\n",
    "\n",
    "feed_height = 1024\n",
    "feed_width = 1536\n",
    "\n",
    "batch_size = 1#0\n",
    "\n",
    "\n",
    "class bokehDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file,root_dir, transform=None):\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        bok = pil.open(self.root_dir + self.data.iloc[idx, 0][1:]).convert('RGB')\n",
    "        org = pil.open(self.root_dir + self.data.iloc[idx, 1][1:]).convert('RGB')\n",
    "\n",
    "\n",
    "        depth_path = str(self.root_dir + self.data.iloc[idx, 1][1:]\n",
    "                         ).replace('original', 'depth').replace('jpg', 'png')\n",
    "        depth = pil.open(depth_path)\n",
    "\n",
    "        blur_6_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_6_3_2')\n",
    "        blur_17_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_17_3_2')\n",
    "        blur_65_path = str(self.root_dir + self.data.iloc[idx, 0][1:]).replace('bokeh', 'lens_blur_65_3_3')\n",
    "\n",
    "# [0.2048, 0.1720, 0.0290, 0.0624, 0.2019, 0.1071, 0.0460, 0.0173, 0.1592, 0.0002]\n",
    "#    org      6      10      10      16      20      30      50     65     100\n",
    "        blur_6_path = pil.open(blur_6_path).convert('RGB')\n",
    "\n",
    "        blur_17_path = pil.open(blur_17_path).convert('RGB')\n",
    "\n",
    "        blur_65_path = pil.open(blur_65_path).convert('RGB')\n",
    "\n",
    "\n",
    "\n",
    "        bok = bok.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "        org = org.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "        depth = depth.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "\n",
    "        blur_6_path = blur_6_path.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "        blur_17_path = blur_17_path.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "        blur_65_path = blur_65_path.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "\n",
    "        # blur_25 = Image.fromarray(lens_blur(np.array(org), radius=5, components=5, exposure_gamma=4))\n",
    "\n",
    "\n",
    "        if self.transform : \n",
    "            bok_dep = self.transform(bok)\n",
    "            org_dep = self.transform(org)\n",
    "            depth_dep = self.transform(depth)\n",
    "\n",
    "            blur_6_path = self.transform(blur_6_path)\n",
    "\n",
    "            blur_17_path = self.transform(blur_17_path)\n",
    "\n",
    "            blur_65_path = self.transform(blur_65_path)\n",
    "        # depth_dep = depth_dep / depth_dep.max()\n",
    "        \n",
    "        stacked_10 = torch.stack([org_dep, blur_6_path, blur_17_path, blur_65_path], dim=0)\n",
    "\n",
    "        return (bok_dep, org_dep, depth_dep, stacked_10) # here\n",
    "\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.ToTensor(),\n",
    "]) \n",
    "\n",
    "\n",
    "transform3 = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(p=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset1 = bokehDataset(csv_file = '../MegaDepth/Bokeh_Data/train.csv', root_dir = '.',transform = transform1)\n",
    "trainset2 = bokehDataset(csv_file = '../MegaDepth/Bokeh_Data/train.csv', root_dir = '.',transform = transform2)\n",
    "trainset3 = bokehDataset(csv_file = '../MegaDepth/Bokeh_Data/train.csv', root_dir = '.',transform = transform3)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([trainset1,trainset2,trainset3]), batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=7)\n",
    "# trainloader = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset([trainset1]), batch_size=batch_size,\n",
    "#                                           shuffle=True, num_workers=4)\n",
    "\n",
    "testset = bokehDataset(csv_file = '../MegaDepth/Bokeh_Data/test.csv',  root_dir = '.', transform = transform1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/294 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024, 1536]) torch.Size([1, 3, 1024, 1536]) torch.Size([1, 1, 1024, 1536]) torch.Size([1, 4, 3, 1024, 1536])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/294 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(tqdm(testloader), 0):\n",
    "    a, b, depth, stacked_10 = data\n",
    "\n",
    "    print(a.shape, b.shape, depth.shape, stacked_10.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "learning_rate = 0.00001\n",
    "                                                  \n",
    "MSE_LossFn = nn.MSELoss()\n",
    "L1_LossFn = nn.L1Loss()\n",
    "\n",
    "# state_dict = torch.load('./weights/run_1_12_0.pt')\n",
    "run_name = '0'\n",
    "start_ep = 0\n",
    "\n",
    "# model = torch.compile(model)\n",
    "# bokehNN = bokehNet(netG).cuda()\n",
    "# bokehNN.load_state_dict(state_dict)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaxkuz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/ubuntu/anaconda3/envs/max_2_0/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/max/image/JPEG/C++_pipeline/zip/depth_estimation/DPT/wandb/run-20230510_200955-Unet_lens_4_lpips_finetune_2_continue_50</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/maxkuz/diploma/runs/Unet_lens_4_lpips_finetune_2_continue_50' target=\"_blank\">Unet_lens_4_lpips_finetune_2_continue_50</a></strong> to <a href='https://wandb.ai/maxkuz/diploma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maxkuz/diploma' target=\"_blank\">https://wandb.ai/maxkuz/diploma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maxkuz/diploma/runs/Unet_lens_4_lpips_finetune_2_continue_50' target=\"_blank\">https://wandb.ai/maxkuz/diploma/runs/Unet_lens_4_lpips_finetune_2_continue_50</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/maxkuz/diploma/runs/Unet_lens_4_lpips_finetune_2_continue_50?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f02035c2fd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "id=\"Unet_lens_4_lpips_finetune_2_continue_50\"\n",
    "import lpips\n",
    "loss_fn_alex = lpips.LPIPS(net='alex').cuda()\n",
    "\n",
    "wandb.init(\n",
    "    project=\"diploma\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.00001\n",
    "    },\n",
    "    # tags=['JPEG/retrain_from_old_jpeg'],\n",
    "    resume=True,\n",
    "    id=id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = torch.load('/home/ubuntu/max/image/JPEG/C++_pipeline/zip/depth_estimation/DPT/weights/top_4_blurs_finetune/56.pt', map_location=torch.device(\"cpu\"))\n",
    "model.load_state_dict(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 6001/13200 [39:38<47:33,  2.52it/s]  \n",
      "100%|██████████| 294/294 [03:35<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation l1 Loss:  0.04840142234247558\n",
      "Validation lpips Loss:  0.10234804713122901 \n",
      "\n",
      "Validation ms Loss:  0.885392712695258 \n",
      "\n",
      "Average weights: tensor([0.2525, 0.1626, 0.3449, 0.2399])\n",
      "\n",
      "Epoch: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 621/13200 [04:11<1:24:57,  2.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/max/image/JPEG/C++_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 148>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_ep, \u001b[39m500\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch:\u001b[39m\u001b[39m\"\u001b[39m, epoch)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m     train(trainloader)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=152'>153</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=153'>154</a>\u001b[0m         val(testloader, epoch)\n",
      "\u001b[1;32m/home/ubuntu/max/image/JPEG/C++_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb Cell 15\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# loss_l1 = L1_LossFn(bok_pred, bok)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# loss = MSE_LossFn(bok_pred, bok)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# loss = loss_lpips * 1 + loss_l1 * 1.7 + loss_ssim * 35\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m (i \u001b[39m%\u001b[39m \u001b[39m500\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/max_2_0/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/max_2_0/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wandb.watch(model, log_freq=15, log='all')\n",
    "\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for i,data in enumerate(tqdm(dataloader), 0):\n",
    "        if i > 6000:\n",
    "            break\n",
    "        bok, org, depth, stacked_10 = data\n",
    "        bok, org, depth = bok.to(device) , org.to(device), depth.to(device, dtype=torch.float)\n",
    "        stacked_10 = stacked_10.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # bok_mask = model.forward(depth)\n",
    "        bok_mask = model.forward(depth, org)\n",
    "\n",
    "        # stacked_images = torch.stack([org, blur_25, blur_45, blur_75], dim=1).cuda()  # [batch_n, 4, 3, h, w]\n",
    "        # stacked_images = torch.stack([org, blur_45], dim=1).cuda()  # [batch_n, 4, 3, h, w]\n",
    "\n",
    "        # Multiply the maps and images using broadcasting\n",
    "        bok_mask = bok_mask.unsqueeze(2)\n",
    "\n",
    "        output_images = torch.mul(bok_mask, stacked_10) # [batch_n, 4, 3, h, w]\n",
    "        bok_pred = torch.sum(output_images, dim=1) # [batch_n, 3, h, w]\n",
    "\n",
    "        # loss_lpips = loss_fn_alex(bok_pred, bok).sum()\n",
    "        # loss_ssim = 1-ssim(bok_pred, bok)\n",
    "\n",
    "        loss = (1-ms_ssim(bok_pred, bok))\n",
    "        # loss_l1 = L1_LossFn(bok_pred, bok)\n",
    "        # loss = MSE_LossFn(bok_pred, bok)\n",
    "\n",
    "        # loss = loss_lpips * 1 + loss_l1 * 1.7 + loss_ssim * 35\n",
    "        running_loss += loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if (i % 500 == 0):\n",
    "            original = wandb.Image(org[0])\n",
    "            pred = wandb.Image(bok_pred[0])\n",
    "            ground_truth = wandb.Image(bok[0])\n",
    "            wandb.log({\n",
    "                \"Train/1_ms_ssim\": loss,\n",
    "                \"Train/loss\": loss,\n",
    "                \"Train/orig\": original,\n",
    "                \"Train/pred\": pred,\n",
    "                \"Train/bokeh\": ground_truth,\n",
    "                \"Train/depth\": wandb.Image(depth[0] / depth[0].max())\n",
    "                })\n",
    "        #     print ('Batch: ',i,'/',len(dataloader),' Loss:', loss.item())\n",
    "\n",
    "    wandb.log({\n",
    "        \"Train/running_loss\": running_loss/len(dataloader)})\n",
    "\n",
    "\n",
    "\n",
    "def val(dataloader, e):\n",
    "    running_l1_loss = 0\n",
    "    running_ms_loss = 0\n",
    "    running_lpips_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        avg_w = torch.zeros((4))\n",
    "        for i,data in enumerate(tqdm(dataloader), 0):\n",
    "\n",
    "            bok, org, depth, stacked_10 = data\n",
    "            bok, org, depth = bok.to(device) , org.to(device), depth.to(device, dtype=torch.float)\n",
    "            stacked_10 = stacked_10.to(device)\n",
    "\n",
    "            # bok_mask = model.forward(depth)\n",
    "            bok_mask = model.forward(depth, org)\n",
    "\n",
    "            # stacked_images = torch.stack([org, blur_25, blur_45, blur_75], dim=1).cuda()  # [batch_n, 4, 3, h, w]\n",
    "            # stacked_images = torch.stack([org, blur_45], dim=1).cuda()  # [batch_n, 4, 3, h, w]\n",
    "\n",
    "\n",
    "            # Multiply the maps and images using broadcasting\n",
    "            bok_mask = bok_mask.unsqueeze(2)\n",
    "            output_images = torch.mul(bok_mask, stacked_10) # [batch_n, 4, 3, h, w]\n",
    "            bok_pred = torch.sum(output_images, dim=1) # [batch_n, 3, h, w]\n",
    "\n",
    "            # l1_loss = L1_LossFn(bok_pred, bok)\n",
    "\n",
    "            l1_loss = L1_LossFn(bok_pred, bok)\n",
    "            ms_loss = ssim(bok_pred * 255, bok * 255)\n",
    "            lpips_loss = loss_fn_alex(bok_pred, bok).sum()\n",
    "\n",
    "            \n",
    "            avg_w += torch.tensor((bok_mask[:, 0, :, :, :].mean(),\n",
    "                bok_mask[:, 1, :, :, :].mean(),\n",
    "                bok_mask[:, 2, :, :, :].mean(),\n",
    "                bok_mask[:, 3, :, :, :].mean(),\n",
    "                ))\n",
    "                \n",
    "\n",
    "            if i % 10 == 0:\n",
    "\n",
    "                original = wandb.Image(org[0])\n",
    "                pred = wandb.Image(bok_pred[0])\n",
    "                ground_truth = wandb.Image(bok[0])\n",
    "\n",
    "                wandb.log({\n",
    "                    \"Val/L1_loss\": l1_loss,\n",
    "                    \"Val/lpips_loss\": lpips_loss,\n",
    "                    \"Val/orig\": original,\n",
    "                    \"Val/pred\": pred,\n",
    "                    \"Val/bokeh\": ground_truth,\n",
    "                    \"Val/depth\": wandb.Image(depth[0]),\n",
    "                    'Val/map': wandb.Image(bok_mask[0]),\n",
    "                    # 'Val/blur_25': wandb.Image(blur_25[0]),\n",
    "                    # 'Val/blur_45': wandb.Image(blur_45[0]),\n",
    "                    # 'Val/blur_75': wandb.Image(blur_75[0])\n",
    "                    })\n",
    "            if i % 30 == 0:\n",
    "                names = ['org', 'blur_6', 'blur_17', 'blur_65']\n",
    "\n",
    "                for i in range(1, len(names)):\n",
    "                    wandb.log({\n",
    "                        f\"Val/{names[i]}\": wandb.Image(stacked_10[0][i])\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            running_l1_loss += l1_loss.item()\n",
    "            running_ms_loss += ms_loss.item()\n",
    "            running_lpips_loss += lpips_loss.item()\n",
    "\n",
    "    wandb.log({\n",
    "        \"Val/L1_loss\": running_l1_loss / len(dataloader),\n",
    "        \"Val/MS_loss\": running_ms_loss / len(dataloader),\n",
    "        \"Val/LPIPS_loss\": running_lpips_loss / len(dataloader),\n",
    "        \"Val/Epoch\": e,\n",
    "        })\n",
    "\n",
    "    print ('Validation l1 Loss: ',running_l1_loss/ len(dataloader))   \n",
    "    print ('Validation lpips Loss: ',running_lpips_loss/ len(dataloader), '\\n')\n",
    "    print ('Validation ms Loss: ',running_ms_loss/ len(dataloader), '\\n')\n",
    "    print(\"Average weights:\", avg_w / len(dataloader))\n",
    "\n",
    "    torch.save(model.state_dict(), f'./weights/top_4_blurs_finetune/{e}.pt')\n",
    "\n",
    "start_ep = 57\n",
    "\n",
    "for epoch in range(start_ep, 500):\n",
    "    print(\"\\nEpoch:\", epoch)\n",
    "\n",
    "    train(trainloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val(testloader, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/max/image/JPEG/C++_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.6.1.32/home/ubuntu/max/image/JPEG/C%2B%2B_pipeline/zip/depth_estimation/DPT/blur_from_depth_model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./weights/top_4_blurs_finetune/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m43\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), f'./weights/top_4_blurs_finetune/{43}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
